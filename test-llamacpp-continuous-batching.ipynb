{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 12:09:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8             11W /  130W |    7080MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1      C   /python3.10                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\"object\":\"model\",\"created\":1714734632,\"owned_by\":\"vllm\",\"root\":\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-d4b644a1e0bb4eeba97216c1e307b60c\",\"object\":\"model_permission\",\"created\":1714734632,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl http://0.0.0.0:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"sk-xxx\") # dummy key\n",
    "\n",
    "\n",
    "def get_response(name, max_tokens = 100, prompt = \"Write a story about llamas.\", system_prompt = \"You are a story writing assistant.\"):\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        model=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ])\n",
    "    return name, response, time.time() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.858847618103027s\n",
      "In the small village of Pachamama, nestled in the Andean mountains, a group of llamas held a special place in the hearts of the villagers. These majestic creatures, with their soft fur and gentle eyes, had been a part of the community for generations.\n",
      "\n",
      "The story begins with a young llama named Luna, who lived on a farm on the outskirts of the village. Luna was the most curious and adventurous of the llamas, always looking for new experiences and exploring the surrounding countryside. One day, while wandering through the nearby forest, Luna stumbled upon a hidden clearing deep in the woods.\n",
      "\n",
      "As she entered the clearing, she noticed a small, intricately carved wooden door hidden behind a waterfall. The door was adorned with symbols and patterns, which Luna didn't recognize. But she felt an inexplicable connection to the door, as if it held a secret just for her.\n",
      "\n",
      "Without hesitation, Luna pushed the door open and discovered a cozy, hidden room filled with books, scrolls, and strange artifacts. The air was thick with the scent of old parchment and the whispers of ancient knowledge. A soft, golden light emanated from a small, ornate lantern placed in the center of the room.\n",
      "\n",
      "As Luna explored the room, she began\n"
     ]
    }
   ],
   "source": [
    "name, response, t = get_response(0, max_tokens=250)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submited thread 0\n",
      "Submited thread 1\n",
      "Submited thread 2\n",
      "Submited thread 3\n",
      "Submited thread 4\n",
      "Submited thread 5\n",
      "Submited thread 6\n",
      "Submited thread 7\n",
      "Submited thread 8\n",
      "Submited thread 9\n",
      "Request 3, time: 6.48, n_tokens = 279\n",
      "Request 6, time: 6.46, n_tokens = 279\n",
      "Request 0, time: 6.50, n_tokens = 279\n",
      "Request 1, time: 6.49, n_tokens = 279\n",
      "Request 9, time: 6.45, n_tokens = 279\n",
      "Request 7, time: 6.46, n_tokens = 279\n",
      "Request 4, time: 6.48, n_tokens = 279\n",
      "Request 2, time: 6.49, n_tokens = 279\n",
      "Request 8, time: 6.46, n_tokens = 279\n",
      "Request 5, time: 6.48, n_tokens = 279\n",
      "Total Execution time: 6.502201795578003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "t0 = time.time()\n",
    "threads = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit the tasks to the executor\n",
    "    for i in range(10):\n",
    "        threads.append(executor.submit(get_response, i, 250))\n",
    "        print('Submited thread', i)\n",
    "\n",
    "    # print results as and when they come in\n",
    "    for task in concurrent.futures.as_completed(threads):\n",
    "        name, response, exec_time = task.result()\n",
    "        message = response.choices[0].message.content\n",
    "        print(f'Request {name}, time: {exec_time:.2f}, n_tokens = {response.usage.total_tokens}')\n",
    "        #print(message)\n",
    "\n",
    "print('Total Execution time:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.888301372528076s\n",
      "Let's break down the problem step by step!\n",
      "\n",
      "1. Calculate the daily driving distance:\n",
      "With 8 hours of driving per day, and a fuel efficiency of 25 miles per gallon, the daily driving distance can be calculated as:\n",
      "\n",
      "Daily driving distance = Fuel efficiency * 8 hours\n",
      "= 25 miles/gallon * 8 hours\n",
      "= 200 miles/day\n",
      "\n",
      "2. Calculate the total driving distance from New York to Los Angeles:\n",
      "Assuming a direct route (which is approximately 2,796 miles), we can calculate the total driving distance:\n",
      "\n",
      "Total driving distance = 2,796 miles\n",
      "\n",
      "3. Calculate the total fuel consumption:\n",
      "Since the car has a 15-gallon gas tank, and the fuel efficiency is 25 miles per gallon, we can calculate the total fuel consumption:\n",
      "\n",
      "Total fuel consumption = Total driving distance / Fuel efficiency\n",
      "= 2,796 miles / 25 miles/gallon\n",
      "= 111.84 gallons\n",
      "\n",
      "4. Calculate the number of days needed for the trip:\n",
      "Since the car has a 15-gallon gas tank, and it needs to stop for gas when it's empty, we can calculate the number of days needed for the trip:\n",
      "\n",
      "Number of days = Total fuel consumption / Daily fuel\n"
     ]
    }
   ],
   "source": [
    "# see response quality\n",
    "prompt = \"\"\"\n",
    "A person is planning a road trip from New York to Los Angeles. \n",
    "They have a car with a 15-gallon gas tank, and the fuel efficiency is 25 miles per gallon. \n",
    "Assuming they drive for 8 hours a day, how many days will the trip take, and how many times will they need to stop for gas? \n",
    "Make assumptions about the route and fuel prices as needed.\n",
    "\"\"\"\n",
    "getname, response, t = get_response(0, max_tokens=250, system_prompt='You are a helpful assistant.', prompt=prompt)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
