{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 13:04:09 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8             14W /  115W |    6056MiB /   8192MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1      C   /server                                     N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"models/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf\",\"object\":\"model\",\"created\":1714737857,\"owned_by\":\"llamacpp\",\"meta\":{\"vocab_type\":2,\"n_vocab\":128256,\"n_ctx_train\":8192,\"n_embd\":4096,\"n_params\":8030261248,\"size\":5725151232}}]}"
     ]
    }
   ],
   "source": [
    "!curl http://0.0.0.0:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"sk-xxx\") # dummy key\n",
    "\n",
    "\n",
    "def get_response(name, max_tokens = 100, prompt = \"Write a story about llamas.\", system_prompt = \"You are a story writing assistant.\"):\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        max_tokens=max_tokens,\n",
    "        model=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ])\n",
    "    return name, response, time.time() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 6.6008758544921875s\n",
      "In the small village of Andesville, nestled in the heart of the Andes Mountains, there lived a group of llamas like no other. These were no ordinary llamas, for they possessed a special gift – the ability to communicate with humans through a unique form of telepathy.\n",
      "\n",
      "The villagers had grown accustomed to the llamas' gentle hums and soft whispers, which seemed to carry a deep wisdom and understanding. They would often sit with the llamas, listening int – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– been;ampSolve the problem by using the following steps:\n",
      "\n",
      "1. Identify the problem: The problem is that the code is not working\n"
     ]
    }
   ],
   "source": [
    "name, response, t = get_response(0, max_tokens=250)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submited thread 0\n",
      "Submited thread 1\n",
      "Submited thread 2\n",
      "Submited thread 3\n",
      "Submited thread 4\n",
      "Submited thread 5\n",
      "Submited thread 6\n",
      "Submited thread 7\n",
      "Submited thread 8\n",
      "Submited thread 9\n",
      "Request 5, time: 8.67, n_tokens = 281\n",
      "Request 3, time: 8.68, n_tokens = 281\n",
      "Request 1, time: 8.69, n_tokens = 281\n",
      "Request 0, time: 8.71, n_tokens = 281\n",
      "Request 4, time: 17.34, n_tokens = 281\n",
      "Request 8, time: 17.33, n_tokens = 281\n",
      "Request 2, time: 17.37, n_tokens = 281\n",
      "Request 9, time: 17.33, n_tokens = 281\n",
      "Request 7, time: 24.05, n_tokens = 281\n",
      "Request 6, time: 24.08, n_tokens = 281\n",
      "Total Execution time: 24.115912199020386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "t0 = time.time()\n",
    "threads = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit the tasks to the executor\n",
    "    for i in range(10):\n",
    "        threads.append(executor.submit(get_response, i, 250))\n",
    "        print('Submited thread', i)\n",
    "\n",
    "    # print results as and when they come in\n",
    "    for task in concurrent.futures.as_completed(threads):\n",
    "        name, response, exec_time = task.result()\n",
    "        message = response.choices[0].message.content\n",
    "        print(f'Request {name}, time: {exec_time:.2f}, n_tokens = {response.usage.total_tokens}')\n",
    "        #print(message)\n",
    "\n",
    "print('Total Execution time:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 6.685110092163086s\n",
      "Let's break down the problem step by step!\n",
      "\n",
      "1. First, let's calculate the daily! \n",
      " ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !;amp;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;\n"
     ]
    }
   ],
   "source": [
    "# see response quality\n",
    "prompt = \"\"\"\n",
    "A person is planning a road trip from New York to Los Angeles. \n",
    "They have a car with a 15-gallon gas tank, and the fuel efficiency is 25 miles per gallon. \n",
    "Assuming they drive for 8 hours a day, how many days will the trip take, and how many times will they need to stop for gas? \n",
    "Make assumptions about the route and fuel prices as needed.\n",
    "\"\"\"\n",
    "getname, response, t = get_response(0, max_tokens=250, system_prompt='You are a helpful assistant.', prompt=prompt)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 6.443597793579102s\n",
      "Let's break down the problem step by step!\n",
      "\n",
      "1. First, let's calculate the daily! \n",
      " ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !;amp;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;#;\n"
     ]
    }
   ],
   "source": [
    "# try again\n",
    "prompt = \"\"\"\n",
    "A person is planning a road trip from New York to Los Angeles. \n",
    "They have a car with a 15-gallon gas tank, and the fuel efficiency is 25 miles per gallon. \n",
    "Assuming they drive for 8 hours a day, how many days will the trip take, and how many times will they need to stop for gas? \n",
    "Make assumptions about the route and fuel prices as needed.\n",
    "\"\"\"\n",
    "getname, response, t = get_response(0, max_tokens=250, system_prompt='You are a helpful assistant.', prompt=prompt)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 6.308935642242432s\n",
      "In the small village of Andesville, nestled in the heart of the Andes Mountains, there lived a group of llamas like no other. These were no ordinary llamas, for they possessed a special gift – the ability to communicate with humans through a unique form of telepathy.\n",
      "\n",
      "The villagers had grown accustomed to the llamas' gentle hums and soft whispers, which seemed to carry a deep wisdom and understanding. They would often sit with the llamas, listening int – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– been;ampSolve the problem by using the following steps:\n",
      "\n",
      "1. Identify the problem: The problem is that the code is not working\n"
     ]
    }
   ],
   "source": [
    "#ok try the llama prompt\n",
    "\n",
    "getname, response, t = get_response(0, max_tokens=250)\n",
    "print(f'Took {t}s')\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
