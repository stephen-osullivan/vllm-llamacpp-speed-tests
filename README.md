# vllm-llamacpp-speed-tests
I'll run vllm and llamacpp using docker on quantized llama3 (awq for vllm and gguf for cpp). I'll send requests to both and check the speed.
